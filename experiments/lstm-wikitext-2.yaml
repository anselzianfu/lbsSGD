dataset: wikitext-2
model: lstm
seed: 1
learning_rate:
    grid_search: [0.1, 1]
linear_lr:
    grid_search: [true, false]
dropout: 0.0
grad_clip: -1.
baseline_bz: 32
batch_size:
    grid_search: [8, 32, 128, 512, 2048]
max_epochs: 100
persist_n: 50
evaluate_n: 20
warmup_epochs: 5
eval_batches: 2
max_samples_per_gpu: 512
